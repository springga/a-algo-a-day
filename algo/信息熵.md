# 信息熵

## 任务

假设你要发送两条信息给对方，都是是10次掷硬币的结果，一个硬币两面不同，一个两面都是一样，两者的分别需要用多少bit表示？
答案是10和0，因为第一个硬币的结果是随机的，第二个硬币的结果是确定的，香农用信息熵来度量不确定的程度，通俗的讲，就是得知结果至少需要问多少个是否问题？第一个硬币的每次结果都要问1个是否问题，因此至少要问10个问题，第二个硬币不需要问就知道结果

## 如何一般地计算最少需要多少个是否问题呢？

假设字母a,b,c,d出现的概率都是1/4，则至少需要2个是否问题才能得知是哪个字母，则每个字母的熵都是2
假设字母a的概率是1/2，b是1/4，c和d都是1/8，则只要1个问题就能确定a，至少2个问题确定b，至少3个问题确定c和d，那么字母的平均长度是1*1/2 + 2*1/4 + 3*1/8 + 3*1/8 = 1.75，字母的熵就是1.75
一般地，概率已知时，信息熵就是加总每个字符出现的概率 * 字符的最小编码长度，sum(Ps * #s)，而每个字符的最小编码长度可以用log(1/Ps)度量，最终熵H(s) = -sum(Ps * logPs)

## 信息熵的特性

举例：
- 一个硬币两面一样，无论怎么投，结果是确定的，这时的信息熵是0，即不确定性为0
- 一个硬币两面不同，每面出现的概率不同，则信息熵在0和1之间，如果每面出现的概率相同，则信息熵是1
- 一个骰子六面不同，每面出现的概率一样，则信息熵>1

总结一下：
- 选项数量不变时，每个选项出现的概率越趋于相同，信息熵越大
- 选项出现的概率相同时，选项越多，信息熵越大

这与我们的直觉相符，情况越混乱，选项越多，不确定性越大
反之，选项越少，可能性越集中，则不确定性越小

## 条件熵

一个变量可以给预测第二个变量带来额外信息，比如打雷就更有可能下雨
如何表示第一个变量存在时，第二个变量的信息熵呢？
条件熵H(y|x) = -sum(P(x,y) * logP(y|x)) = sum(P(x) * H(y|X=x))

决策树算法就是用条件熵来计算信息增益的