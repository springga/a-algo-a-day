# 决策树

## 任务

以检测信用卡欺诈为例，你有很多数据，包含用卡人的各种身份和行为特征，哪些特征能识别出欺诈行为呢？
当然，理想的情况是有一种特征正好跟欺诈行为对应，一击命中，然而，现实是需要多种特征组合，还不能保证100%准确，那如何挑选出这些特征，如何组合，尽量提高准确率和降低误检率呢？

## 思路

手工做的话，如同破案一样，应该先找跟欺诈行为最相关的特征，当然这不足以完全区分欺诈和非欺诈行为，但可以大大缩小目标范围，下一步再找另一个特征，进一步缩小范围，重复这一步骤，直到掌握的数据无法进一步缩小范围

## ID3

程序做的话，步骤是一样的，关键是如何量化特征与目标的相关性，这里的相关，是能尽量确定就是欺诈行为，如何衡量确定性呢？香农在1948就提出了信息熵，用来衡量信息量，也可以用来衡量不确定性，为什么能衡量后面单独解释，总之信息熵越大，则越不确定，如果一个特征能减少目标的信息熵，减少的量表示为信息增益，那么只要找到信息增益最大的特征，就能最大程度地确定目标，缩小目标范围，按这个思路找下一个特征，直到信息增益为0，这个算法就是Quinlan在1986提出的ID3(Iterative Dichotomiser 3，迭代二分法)

## C4.5

但ID3有一个缺陷，跟信息熵的特性有关，假如一个特征只有一个值，显然对确定目标没有帮助，信息增益为0，而如果特征的每个值都不同，那只要把与目标对应的值找出来，就能确定目标，一击命中，信息增益最大，但这样的特征可能并无意义，比如编号，对以后的预测也不会有帮助，为了避免ID3算法倾向于值多样的特征，再引入一个衡量值多样性的指标，比如特征本身的信息熵，等一下，信息熵怎么还能衡量多样性？因为值越多样，信息熵越大，这个特性后面再展开解释，用信息增益除以特征本身的信息熵，就平衡了对值多样的偏好，这个改进的指标叫信息增益率，利用这个指标，Quinlan在1993提出C4.5，但不是简单地替换指标，而是先用ID3生成规则，再利用信息增益率决定规则的次序，同时也改进了ID3只能应用于类别数据的局限，通过自动把连续值分段，把连续值转换为离散值再处理

## CART

衡量确定性的指标不止信息熵，还有基尼系数，对的，就是度量不平等程度的基尼系数，在其他教程里，经常把信息熵和基尼系数作为不纯度的指标，因为它们具有相同的特性：
- 选项越多，指标越大，越不纯
- 可能性越集中，指标越小，越纯

但这跟确定目标有什么关系？换个说法会更容易理解，缩小范围，确定目标的过程，也可以理解成提纯的过程，逐步把杂质去除，最后只剩下目标，也就确定了目标，特征如果提高了目标的纯度，也就提高了确定性
而使用基尼系数的算法就叫CART(classification and regression tree)，由Breiman等人在1984年提出，不仅能用于分类，还能用于回归

## 实战经验

- 先对特征降维可以让决策树找到更具辨识度的特征
- 分类不均衡的数据，可以通过采样均衡，以免决策树偏向样本多的分类

过拟合是几乎每个机器学习算法都会碰到的问题，决策树如何防止过拟合呢？
- 注意样本和特征的比例，样本少但特征多，就容易过拟合
- 控制树的深度，可以先设置深度为3，可视化看看结果，觉得靠谱再增加深度，通常每增加一层，要求样本数翻倍
- 控制每个节点的最少样本数，过拟合通常会导致节点样本过少，分类本身不均衡可以按比例控制，如果分类少可以不控制

## 参考

https://scikit-learn.org/stable/modules/tree.html